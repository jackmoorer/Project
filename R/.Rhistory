#report results from the native country table
sink("../output/EDA_results/freq-table-train-data.txt")
table(train$native_country)
sink()
#report plot of frequencies showing how overwhelming US majority is
pdf("../images/EDA_plots/tain-data-country-freq-plot.pdf")
ggplot(data = native_country, aes(x = Var1, y = Freq)) + geom_bar(stat = "identity") + ggtitle("Frequency Plot of Native Countries for Training Data")
dev.off()
#create dumby variable for training data
us_citizen <- rep(0, nrow(train))
us_citizen[train$native_country == "United-States"] <- "Yes"
us_citizen[train$native_country != "United-States"] <- "No"
#add dumby variable
train$US_Citizen <- as.factor(us_citizen)
#create dumby varaible for test data
us_citizen_test <- rep(0, nrow(test))
us_citizen_test[test$native_country == "United-States"] <- "Yes"
us_citizen_test[test$native_country != "United-States"] <- "No"
#add dumby variable
test$US_Citizen <- as.factor(us_citizen_test)
#remove original native_countries
train <- train[, -14]
test <- test[, -14]
#create numeric response variable for train
over50k_numeric_train <- rep(0, nrow(train))
over50k_numeric_train[train$Over50k == ">50K"] <- 1
train$over50k_numeric <- over50k_numeric_train
#create numeric response variable for test
over50k_numeric_test <- rep(0, nrow(test))
over50k_numeric_test[test$Over50k == ">50K."] <- 1
test$over50k_numeric <- over50k_numeric_test
#change train response column Over50k to yes or no factor
train_response <- train$over50k_numeric
train_response[train$over50k_numeric == 1] <- "Yes"
train_response[train$over50k_numeric == 0] <- "No"
train$Over50k <- as.factor(train_response)
#change test response column Over50k to yes or no factor
test_response <- test$over50k_numeric
test_response[test$over50k_numeric == 1] <- "Yes"
test_response[test$over50k_numeric == 0] <- "No"
test$Over50k <- as.factor(test_response)
#lets look that the correlation of the varibles using hetcor() from the polycor package
library(polycor)
#report correlation
sink("../output/EDA_results/training-correlation.txt")
hetcor(train)
getwd()
setwd("/Users/jackmoorer/Stat154/Projects/Project/R/")
#read in training data
train <- read.csv("../data/adult.csv", header = FALSE, strip.white=TRUE,
stringsAsFactors = FALSE)
#read in test data
test <- read.csv("../data/test.csv", header = FALSE, strip.white=TRUE,
stringsAsFactors = FALSE)
#create vector of column names
names <- c("age", "workclass", "fnlwgt", "education", "education_num",
"marital_status", "occupation", "relationship", "race", "sex",
"capital_gain", "capital_loss", "hours_per_week", "native_country",
"Over50k")
#add column names to train
names(train) <- names
#add column names to test
names(test) <- names
#convert age to a numeric value instead of factor
train$age <- as.numeric(train$age)
test$age <- as.numeric(test$age)
#convert "?" to NA
train[train == "?"] <- NA
test[test == "?"] <- NA
#convert string columns to factor columns for train
train$workclass <- as.factor(train$workclass)
train$education <- as.factor(train$education)
train$marital_status <- as.factor(train$marital_status)
train$occupation <- as.factor(train$occupation)
train$relationship <- as.factor(train$relationship)
train$race <- as.factor(train$race)
train$sex <- as.factor(train$sex)
#convert string columns to factor columns for test
test$workclass <- as.factor(test$workclass)
test$education <- as.factor(test$education)
test$marital_status <- as.factor(test$marital_status)
test$occupation <- as.factor(test$occupation)
test$relationship <- as.factor(test$relationship)
test$race <- as.factor(test$race)
test$sex <- as.factor(test$sex)
#remove na values from train
train <- na.omit(train)
#remove na values from test
test <- na.omit(test)
#We decided to omit native country for several reasons.
#One is that in general, when we kept native country, it was one of the least important variables
#The other is it was hard to deal with since tree could not accept a factor with more than 32 levels
#Also the test set had one country not in the training set
#Instead we are going to make a variable "US_Citizen" that indicates whether an individual is a US citizen or not
#We did this because the vast majority of people are US Citizens
library(ggplot2)
native_country <- data.frame(table(train$native_country))
#report results from the native country table
sink("../output/EDA_results/freq-table-train-data.txt")
table(train$native_country)
sink()
#report plot of frequencies showing how overwhelming US majority is
pdf("../images/EDA_plots/tain-data-country-freq-plot.pdf")
ggplot(data = native_country, aes(x = Var1, y = Freq)) + geom_bar(stat = "identity") + ggtitle("Frequency Plot of Native Countries for Training Data")
dev.off()
#create dumby variable for training data
us_citizen <- rep(0, nrow(train))
us_citizen[train$native_country == "United-States"] <- "Yes"
us_citizen[train$native_country != "United-States"] <- "No"
#add dumby variable
train$US_Citizen <- as.factor(us_citizen)
#create dumby varaible for test data
us_citizen_test <- rep(0, nrow(test))
us_citizen_test[test$native_country == "United-States"] <- "Yes"
us_citizen_test[test$native_country != "United-States"] <- "No"
#add dumby variable
test$US_Citizen <- as.factor(us_citizen_test)
#remove original native_countries
train <- train[, -14]
test <- test[, -14]
#create numeric response variable for train
over50k_numeric_train <- rep(0, nrow(train))
over50k_numeric_train[train$Over50k == ">50K"] <- 1
train$over50k_numeric <- over50k_numeric_train
#create numeric response variable for test
over50k_numeric_test <- rep(0, nrow(test))
over50k_numeric_test[test$Over50k == ">50K."] <- 1
test$over50k_numeric <- over50k_numeric_test
#change train response column Over50k to yes or no factor
train_response <- train$over50k_numeric
train_response[train$over50k_numeric == 1] <- "Yes"
train_response[train$over50k_numeric == 0] <- "No"
train$Over50k <- as.factor(train_response)
#change test response column Over50k to yes or no factor
test_response <- test$over50k_numeric
test_response[test$over50k_numeric == 1] <- "Yes"
test_response[test$over50k_numeric == 0] <- "No"
test$Over50k <- as.factor(test_response)
#lets look that the correlation of the varibles using hetcor() from the polycor package
library(polycor)
#report correlation
sink("../output/EDA_results/training-correlation.txt")
hetcor(train)
sink()
#reorder the data frames and remove education_num
train <- train[c("age", "workclass", "fnlwgt", "education",
"marital_status", "occupation", "relationship", "race", "sex",
"capital_gain", "capital_loss", "hours_per_week", "US_Citizen",
"Over50k", "over50k_numeric")]
test <- test[c("age", "workclass", "fnlwgt", "education",
"marital_status", "occupation", "relationship", "race", "sex",
"capital_gain", "capital_loss", "hours_per_week", "US_Citizen",
"Over50k", "over50k_numeric")]
#write clean test csv file
write.csv(test, file = "../data/clean_test.csv", row.names = FALSE)
#write clean train csv file
write.csv(train, file = "../data/clean_train.csv", row.names = FALSE)
#read in the clean data
train <- read.csv("/Users/jackmoorer/Stat154/Projects/Project/data/clean_train.csv", header = TRUE)
test <- read.csv("/Users/jackmoorer/Stat154/Projects/Project/data/clean_test.csv", header = TRUE)
test
#remove numeric reponse column
train_tree <- train[,-ncol(train)]
#default classification tree
classification_tree <- tree(Over50k ~., data = train_tree)
summary(classification_tree)
#tune parameters
set.seed(100)
classification_tree_cv <- cv.tree(classification_tree, FUN = prune.misclass)
#look at data frame of cv results
library(ggplot2)
Size <- classification_tree_cv$size
K <- classification_tree_cv$k
Dev <- classification_tree_cv$dev
Misclass <- data.frame(Size, K, Dev)
Misclass
#plot cv results of number of terminal nodes
ggplot(data = Misclass, aes(x = Size, y = Dev)) + geom_point() + geom_line() + ggtitle("Size of Tree vs Error for Misclass Method")
#cv using prune.tree
set.seed(200)
classification_tree_cv_default <- cv.tree(classification_tree, FUN = prune.tree)
#plot cv results for size
ggplot(data = default, aes(x = Size, y = Dev)) + geom_point() + geom_line() + ggtitle("Size of Tree vs Error for prune.tree Method")
#creat data frame of results
Size <- classification_tree_cv_default$size
K <- classification_tree_cv_default$k
Dev <- classification_tree_cv_default$dev
default <- data.frame(Size, K, Dev)
default
#plot cv results for size
ggplot(data = default, aes(x = Size, y = Dev)) + geom_point() + geom_line() + ggtitle("Size of Tree vs Error for prune.tree Method")
#plot cv results for cross complexity
ggplot(data = default, aes(x = K, y = Dev)) + geom_point() + geom_line() + ggtitle("Cost-Complexity vs Error for Default Method")
#get best size parameter from cross validation
names <- classification_tree_cv_default$size
values <- classification_tree_cv_default$dev
names(values) <- names
size <- as.numeric(names(which.min(values)))
#build classification tree
set.seed(4)
prune_classification_tree <- prune.misclass(classification_tree, best = 8)
#let's look at the tree
prune_classification_tree
#lets look at the structure of the tree
plot(prune_classification_tree)
text(prune_classification_tree, pretty = 0)
#build classification tree
set.seed(4)
prune_classification_tree <- prune.misclass(classification_tree, best = 5)
#lets look at the structure of the tree
plot(prune_classification_tree)
text(prune_classification_tree, pretty = 0)
#build classification tree
set.seed(4)
prune_classification_tree <- prune.misclass(classification_tree, best = size)
#let's look at the tree
prune_classification_tree
#lets look at the structure of the tree
plot(prune_classification_tree)
text(prune_classification_tree, pretty = 0)
#get error rate of classification tree
err_rate <- mean(train_preds != real_train)
err_rate
#compute confusion matrix
real_train <- train_tree$Over50k
train_preds <- predict(prune_classification_tree, train_tree, type = "class")
table(train_preds, real_train)
#get error rate of classification tree
err_rate <- mean(train_preds != real_train)
err_rate
#get accuracy
training_accuracy_class_tree = 1 - err_rate
#load package
library(ROCR)
#prepare roc plot
train_probs <- predict(prune_classification_tree, train_tree)
train_prediction <- prediction(train_probs[,2], real_train)
train_performance <- performance(train_prediction, measure = "tpr", x.measure = "fpr")
#plot roc
plot(train_performance, main = "Train ROC Curve for Classification Tree")
abline(a=0, b=1, lty=2)
#get train auc
class_tree_train_auc <- performance(train_prediction, measure="auc")@y.values[[1]]
class_tree_train_auc
#prepare test data
test_tree <- test[, -ncol(test)]
test_tree_preds <- test_tree[, -ncol(test_tree)]
real_test <- test_tree$Over50k
#get error rate
ct_test_preds <- predict(prune_classification_tree, test_tree_preds, type = "class")
ct_test_err_rate <- mean(ct_test_preds != real_test)
ct_test_err_rate
#get error rate
ct_test_preds <- predict(prune_classification_tree, test_tree_preds, type = "class")
ct_test_err_rate <- mean(ct_test_preds != real_test)
ct_test_err_rate
#get test accuracy
ct_test_acc <- 1 - ct_test_err_rate
ct_test_acc
#get classification tree confusion matrix
class_tree_confusionMatrix <- table(ct_test_preds, real_test)
class_tree_confusionMatrix
#prepare test data
test_tree <- test[, -ncol(test)]
test_tree_preds <- test_tree[, -ncol(test_tree)]
real_test <- test_tree$Over50k
#get error rate
ct_test_preds <- predict(prune_classification_tree, test_tree_preds, type = "class")
ct_test_err_rate <- mean(ct_test_preds != real_test)
ct_test_err_rate
#get test accuracy
ct_test_acc <- 1 - ct_test_err_rate
ct_test_acc
#get classification tree confusion matrix
class_tree_confusionMatrix <- table(ct_test_preds, real_test)
class_tree_confusionMatrix
#calculate sensitivity
sensitivity <- confusionMatrix[2, 2]/(confusionMatrix[2, 2] + confusionMatrix[1, 2])
#calculate sensitivity
sensitivity <- class_tree_confusionMatrix[2, 2]/(confusionMatrix[2, 2] + confusionMatrix[1, 2])
#calculate sensitivity
sensitivity <- class_tree_confusionMatrix[2, 2]/(class_tree_confusionMatrix[2, 2] + class_tree_confusionMatrix[1, 2])
sensitivity
#calculate specificity
specificity <- class_tree_confusionMatrix[1, 1]/(class_tree_confusionMatrix[1, 1] + class_tree_confusionMatrix[2, 1])
specificity
#calculate sensitivity
ct_sensitivity <- class_tree_confusionMatrix[2, 2]/(class_tree_confusionMatrix[2, 2] + class_tree_confusionMatrix[1, 2])
ct_sensitivity
#calculate specificity
ct_specificity <- class_tree_confusionMatrix[1, 1]/(class_tree_confusionMatrix[1, 1] + class_tree_confusionMatrix[2, 1])
ct_specificity
#prepare roc curve
ct_test_probs <- predict(prune_classification_tree, test_forest_preds)
#prepare roc curve
ct_test_probs <- predict(prune_classification_tree, test_tree_preds)
ct_test_prediction <- prediction(ct_test_probs[,2], real_test)
ct_test_performance <- performance(ct_test_prediction,  measure = "tpr", x.measure = "fpr")
#plot roc curve
plot(ct_test_performance, main="Test ROC Classification Tree")
abline(a=0, b=1, lty=2)
#report test auc
auc <- performance(ct_test_prediction, measure="auc")@y.values[[1]]
auc
#report test auc
ct_test_auc <- performance(ct_test_prediction, measure="auc")@y.values[[1]]
ct_test_auc
#set seed
set.seed(200)
#number of trees to look at
ntree <- c(50, 100, 500, 1000, 1500)
#how I will store the results
cv_list <- as.list(rep(0, 5))
names(cv_list) <- ntree
i <- 1
#loop through number of trees and run tuneRF to look at combination of mtyr and number of trees
for (tree in ntree) {
tune_param <- tuneRF(train_predictors, train_response, ntreeTry = tree, trace = FALSE, plot = FALSE)
cv_list[[i]] <- tune_param
i <- i + 1
}
#load packages
library(randomForest)
library(caret)
#set seed
set.seed(200)
#number of trees to look at
ntree <- c(50, 100, 500, 1000, 1500)
#how I will store the results
cv_list <- as.list(rep(0, 5))
names(cv_list) <- ntree
i <- 1
#loop through number of trees and run tuneRF to look at combination of mtyr and number of trees
for (tree in ntree) {
tune_param <- tuneRF(train_predictors, train_response, ntreeTry = tree, trace = FALSE, plot = FALSE)
cv_list[[i]] <- tune_param
i <- i + 1
}
#set seed
set.seed(200)
#number of trees to look at
ntree <- c(50, 100, 500, 1000, 1500)
#how I will store the results
cv_list <- as.list(rep(0, 5))
names(cv_list) <- ntree
i <- 1
#loop through number of trees and run tuneRF to look at combination of mtyr and number of trees
for (tree in ntree) {
tune_param <- tuneRF(train_predictors, train_response, ntreeTry = tree, trace = FALSE, plot = FALSE)
cv_list[[i]] <- tune_param
i <- i + 1
}
#seperate predictors and response
train_predictors <- train_forest[, -ncol(train_forest)]
#prepare data
train_forest <- train[, -ncol(train)]
#seperate predictors and response
train_predictors <- train_forest[, -ncol(train_forest)]
train_response <- train_forest$Over50k
set.seed(200)
rf_cv <- rfcv(train_predictors, train_response, cv.fold = 5)
#plot results
with(rf_cv, plot(n.var, error.cv, log="x", type="o", lwd=1, main = "CV Variables vs Error Rate", xlab = "Variables Used", ylab = "CV Error Rate"))
#plot results
with(rf_cv, plot(n.var, error.cv, log="x", type="o", lwd=1, main = "CV Variables vs Error Rate", xlab = "Variables Used", ylab = "CV Error Rate Using rfcv"))
#plot results
with(rf_cv, plot(n.var, error.cv, log="x", type="o", lwd=1, main = "CV Variables vs Error Rate Using rfcv", xlab = "Variables Used", ylab = "CV Error Rate"))
#set seed
set.seed(200)
#number of trees to look at
ntree <- c(50, 100, 500, 1000, 1500)
#how I will store the results
cv_list <- as.list(rep(0, 5))
names(cv_list) <- ntree
i <- 1
#loop through number of trees and run tuneRF to look at combination of mtyr and number of trees
for (tree in ntree) {
tune_param <- tuneRF(train_predictors, train_response, ntreeTry = tree, trace = FALSE, plot = FALSE)
cv_list[[i]] <- tune_param
i <- i + 1
}
#prepare cv results for plotting
library(stringr)
matrix <- matrix(rep(0, 3*5*3), nrow = 15, ncol = 3)
index <- 0
for (i in 1:5) {
cur_cv <- cv_list[i]
errs <- cv_list[[i]][,2]
for (j in 1:length(errs)){
cur_err <- cv_list[[i]][,2][[j]]
val <- str_sub(names(cv_list[[i]][,2][j]), 1, 1)
val <- as.numeric(val)
matrix[index + j, ] <- c(ntree[i], val, cur_err)
}
index <- index + j
}
cv_df <- data.frame(matrix)
names(cv_df) <- c("ntree", "num_var", "OOB")
cv_df
cv_df$number_of_trees <- factor(cv_df$ntree)
ggplot(data = cv_df, aes(x = num_var, y = OOB, color = number_of_trees)) + geom_line() + ggtitle("Random Forest Hyper Parameter Tuning Grouped By Number of Trees")
cv_df$mtyr <- factor(cv_df$num_var)
ggplot(data = cv_df, aes(x = ntree, y = OOB, color = mtyr)) + geom_line() + ggtitle("Random Forest Hyper Parameter Tuning Grouped By mtyr")
oobs <- cv_df$OOB
row_num <- as.numeric(which.min(oobs))
num_tree <- cv_df[row_num, 1]
num_var <- cv_df[row_num, 2]
num_tree
oobs <- cv_df$OOB
row_num <- as.numeric(which.min(oobs))
num_tree <- cv_df[row_num, 1]
num_var <- cv_df[row_num, 2]
num_Var
oobs <- cv_df$OOB
row_num <- as.numeric(which.min(oobs))
num_tree <- cv_df[row_num, 1]
num_var <- cv_df[row_num, 2]
num_var
oobs <- cv_df$OOB
row_num <- as.numeric(which.min(oobs))
num_tree <- cv_df[row_num, 1]
num_var <- cv_df[row_num, 2]
set.seed(100)
random_forest <- randomForest(Over50k ~., data = train_forest, ntree = num_tree, mtry = num_var, importance = TRUE)
#look at variable importance of random forest
importance(random_forest)
#lets plot the variable importance based on mean decrease gini and mean decrease accuracy
varImpPlot(random_forest)
#look at variable importance of random forest
var_imp <- importance(random_forest)
var_imp
#lets plot the variable importance based on mean decrease gini and mean decrease accuracy
varImpPlot(random_forest)
#look at variable importance of random forest
var_imp <- importance(random_forest)
var_imp
rf_var_imp <- data.frame(var_imp)
rf_var_imp
rf_var_imp <- data.frame(var_imp)
rf_var_imp_results <- rf_var_imp[, ncol(rf_var_imp)]
rf_var_imp_results
rf_var_imp <- data.frame(var_imp)
rf_var_imp_results <- rf_var_imp[, c(1, ncol(rf_var_imp))]
rf_var_imp_results
rf_var_imp <- data.frame(var_imp)
rf_var_imp_results <- data.frame(rf_var_imp$MeanDecreaseAccuracy)
rf_var_imp_results
#look at variable importance of random forest
var_imp <- data.frame(importance(random_forest))
var_imp
var_imp_best_6 <- var_imp[order(var_imp$MeanDecreaseAccuracy, decreasing = TRUE)[1:6], ]
var_imp_best_6
var_imp_best_6 <- var_imp[order(var_imp$MeanDecreaseAccuracy, decreasing = TRUE)[1:6], ncol(var_imp)]
var_imp_best_6 <- var_imp[order(var_imp$MeanDecreaseAccuracy, decreasing = TRUE)[1:6], ncol(var_imp)]
var_imp_best_6
var_imp_best_6 <- var_imp[order(var_imp$MeanDecreaseAccuracy, decreasing = TRUE)[1:6],]
var_imp_best_6
var_imp_best_6 <- var_imp[order(var_imp$MeanDecreaseGini, decreasing = TRUE)[1:6],]
var_imp_best_6
#prediction the values of the training set
rf_train_pred <- predict(random_forest, train_forest[, -ncol(train_forest)])
real_train <- train_forest$Over50k
rf_train_err_rate <- mean(rf_train_pred != real_train)
rf_train_err_rate
rf_train_acc <- 1 - rf_err_rate
rf_train_acc <- 1 - rf_train_err_rate
rf_train_acc
#prepare trianing roc
rf_train_probs <- as.vector(random_forest$votes[,2])
rf_prediction <- prediction(rf_train_probs, real_train)
rf_performance <- performance(rf_predictio, measure = "tpr", x.measure = "fpr")
#prepare trianing roc
rf_train_probs <- as.vector(random_forest$votes[,2])
rf_prediction <- prediction(rf_train_probs, real_train)
rf_performance <- performance(rf_prediction, measure = "tpr", x.measure = "fpr")
#plot training roc
plot(rf_performance, main="Training ROC Random Forest")
abline(a=0, b=1, lty = 2)
train_rf_auc <- performance(rf_prediction, measure="auc")@y.values[[1]]
train_rf_auc
print(random_forest$confusion)
#prepare data
test_forest <- test[, -ncol(test)]
test_forest_preds <- test_forest[, -ncol(test_forest)]
real_test <- test_forest$Over50k
rf_test_preds <- predict(random_forest, test_forest_preds)
rf_test_err_rate <- mean(rf_test_preds != real_test)
rf_test_err_rate
rf_test_acc <- 1 - rf_test_err_rate
rf_test_acc
#get test accuracy
ct_test_acc <- 1 - ct_test_err_rate
ct_test_acc
rf_test_confusionMatrix <- table(rf_test_preds, real_test)
rf_test_confusionMatrix
rf_test_sensitivity <- rf_test_confusionMatrix[2, 2]/(rf_test_confusionMatrix[2, 2] + rf_test_confusionMatrix[1, 2])
rf_test_sensitivity
rf_test_specificity <- rf_test_confusionMatrix[1, 1]/(rf_test_confusionMatrix[1, 1] + rf_test_confusionMatrix[2, 1])
rf_test_specificity
#prepare roc cruve
rf_test_probs <- predict(random_forest, test_forest_preds, type = "prob")
rf_test_prediction <- prediction(rf_test_probs[,2], real_test)
rf_test_performance <- performance(rf_test_prediction,  measure = "tpr", x.measure = "fpr")
#plot random forest roc
plot(rf_test_performance, main="Test ROC Random Forest")
abline(a=0, b=1, lty=2)
#get test auc
rf_test_auc <- performance(rf_test_prediction, measure="auc")@y.values[[1]]
rf_test_auc
#report test auc
ct_test_auc <- performance(ct_test_prediction, measure="auc")@y.values[[1]]
ct_test_auc
