row_num <- as.numeric(which.min(oobs))
num_tree <- (cv_df[row_num, 1]
num_var <- cv_df[row_num, 2]
oobs <- cv_df$OOB
row_num <- as.numeric(which.min(oobs))
num_tree <- cv_df[row_num, 1]
num_var <- cv_df[row_num, 2]
num_tree
oobs <- cv_df$OOB
row_num <- as.numeric(which.min(oobs))
num_tree <- cv_df[row_num, 1]
num_var <- cv_df[row_num, 2]
num_tree
num_var
set.seed(100)
random_forest <- randomForest(Over50k ~., data = train_forest, ntree = num_tree, mtry = num_var, importance = TRUE)
print(random_forest)
importance(random_forest)
varImpPlot(random_forest)
rf_pred <- predict(random_forest, train_forest[, -ncol(train_forest)])
real_train <- train_forest$Over50k
err_rate <- mean(rf_pred != real_train)
err_rate
library(ROCR)
probs <- as.vector(random_forest$votes[,2])
prediction <- prediction(probs, real_train)
performance <- performance(prediction, measure = "tpr", x.measure = "fpr")
plot(performance, main="ROC Random Forest")
abline(a=0, b=1, lty = 2)
auc <- performance(prediction, measure="auc")@y.values[[1]]
auc
print(random_forest$confusion)
test_forest <- test[, -ncol(test)]
#load packages
library(ggplot2)
library(randomForest)
library(ROCR)
#read in clean data
train <- read.csv("../data/clean_train.csv", header = TRUE)
#remove numeric predictor
train_tree <- train[,-ncol(train)]
#intialize basic classification tree
classification_tree <- tree(Over50k ~., data = train_tree)
#cross validation based on prune.misclass
set.seed(100)
classification_tree_cv <- cv.tree(classification_tree, FUN = prune.misclass)
#report cross validation for prune.misclass
sink("../output/training_results/cv-prune-misclass-classification-tree.txt")
print(classification_tree_cv)
sink()
#prepare cv plots
Size <- classification_tree_cv_misclass$size
K <- classification_tree_cv_misclass$k
Dev <- classification_tree_cv_misclass$dev
Misclass <- data.frame(Size, K, Dev)
#report cv plot for size
pdf("../images/training_plots/cv-prine-misclass-size-vs-error.pdf")
ggplot(data = Misclass, aes(x = Size, y = Dev)) + geom_point() + geom_line() + ggtitle("Size of Tree vs Error for CV Misclass")
dev.off()
#report cv plot for cost complexicity
pdf("../images/training_plots/cv-prine-misclass-k-vs-error.pdf")
ggplot(data = Misclass, aes(x = K, y = Dev)) + geom_point() + geom_line() + ggtitle("Cost-Complexity vs Error for CV Misclass")
dev.off()
#use default method for cv
set.seed(200)
classification_tree_cv_default <- cv.tree(classification_tree, FUN = prune.tree)
#report cross validation for prune.misclass
sink("../output/training_results/cv-prune-tree-classification-tree.txt")
print(classification_tree_cv_default)
sink()
#prepare cv plots
Size <- classification_tree_cv_default$size
K <- classification_tree_cv_default$k
Dev <- classification_tree_cv_default$dev
default <- data.frame(Size, K, Dev)
#report cv plot for size
pdf("../images/training_plots/cv-prine-tree-size-vs-error.pdf")
ggplot(data = default, aes(x = Size, y = Dev)) + geom_point() + geom_line() + ggtitle("Size of Tree vs Error for CV Default Method")
dev.off()
#report cv plot for cost complexicity
pdf("../images/training_plots/cv-prine-tree-k-vs-error.pdf")
ggplot(data = default, aes(x = K, y = Dev)) + geom_point() + geom_line() + ggtitle("Cost-Complexity vs Error for Default Method")
dev.off()
#get hyper-parameter size
names <- classification_tree_cv_default$size
values <- classification_tree_cv_default$dev
names(values) <- names
size <- as.numeric(names(which.min(values)))
#build tree
set.seed(4)
prune_classification_tree <- prune.misclass(classification_tree, best = size)
#report results of tree
sink("../output/training_results/pruned-classification-tree.txt")
print(prune_classification_tree)
print(" ")
print(summary(prune_classification_tree))
sink()
#show classification tree plot
pdf("../images/training_plots/classification-tree-plot.pdf")
plot(prune_classification_tree)
text(prune_classification_tree, pretty = 0)
dev.off()
#build confusion matrix
real_train <- train_tree$Over50k
train_preds <- predict(prune_classification_tree, train_tree, type = "class")
#report confusion matrix
sink("../output/training_results/classification-tree-train-confusion-matrix.txt")
print(table(train_preds, real_train))
sink()
#get error rate
err_rate <- mean(train_preds != real_train)
#report error rate
sink("../output/training_results/train-error-rate-classification-tree.txt")
print("Train Error Rate for Classification Tree")
print(err_rate)
sink()
#prepare roc
train_probs <- predict(prune_classification_tree, train_tree)
train_prediction <- prediction(train_probs[,2], real_train)
train_performance <- performance(train_prediction, measure = "tpr", x.measure = "fpr")
pdf("../images/training_plots/train-ROC-classificaiotn-tree.pdf")
plot(train_performance, main = "Train ROC Curve for Classification Tree")
abline(a=0, b=1, lty=2)
dev.off()
#get auc
auc <- performance(train_prediction, measure="auc")@y.values[[1]]
#report auc
sink("../output/training_results/classification-tree-train-auc.txt")
print("Train AUC for Classifcation Tree")
print(auc)
sink()
#now moving on to the test data
test_tree <- test[, -ncol(test)]
test_tree_preds <- test_tree[, -ncol(test_tree)]
real_test <- test_tree$Over50k
#get test error rate
ct_test_preds <- predict(prune_classification_tree, test_tree_preds, type = "class")
test_err_rate <- mean(ct_test_preds != real_test)
#report test error rate
sink("../output/test_results/classificaton-tree-test-error-rate.txt")
print("Classification tree test error rate")
print(test_err_rate)
sink()
#create test confusion matrix
confusionMatrix <- table(ct_test_preds, real_test)
#report test confusion matrix
sink("../output/test_results/classification-tree-test-confusion-matrix.txt")
print("Classification Tree Confusion Matrix")
print(confusionMatrix )
sink()
#get sensitivity and specificity
sensitivity <- confusionMatrix[2, 2]/(confusionMatrix[2, 2] + confusionMatrix[1, 2])
specificity <- confusionMatrix[1, 1]/(confusionMatrix[1, 1] + confusionMatrix[2, 1])
#report sensitivity and specificity
sink("../output/test_results/classification-tree-sensitivity-specificity.txt")
print("Classification Tree Sensitivity:")
print(sensitivity)
print("Classification Tree Specificity:")
print(specificity)
sink()
#prepare roc cruve
ct_test_probs <- predict(prune_classification_tree, test_forest_preds)
ct_test_prediction <- prediction(ct_test_probs[,2], real_test)
ct_test_performance <- performance(ct_test_prediction,  measure = "tpr", x.measure = "fpr")
#plot roc
pdf("../images/test_plots/classification-tree-test-roc-curve.pdf")
plot(ct_test_performance, main="Test ROC Classification Tree")
abline(a=0, b=1, lty=2)
dev.off()
knitr::opts_chunk$set(echo = TRUE)
train <- read.csv("/Users/jackmoorer/Stat154/Projects/Project/data/clean_train.csv", header = TRUE)
test <- read.csv("/Users/jackmoorer/Stat154/Projects/Project/data/clean_test.csv", header = TRUE)
train
test
#read in data
test <- read.csv("../data/clean_test.csv", header = TRUE)
#prepare data
test_tree <- test[, -ncol(test)]
test_tree_preds <- test_tree[, -ncol(test_tree)]
real_test <- test_tree$Over50k
#get test error rate
ct_test_preds <- predict(prune_classification_tree, test_tree_preds, type = "class")
test_err_rate <- mean(ct_test_preds != real_test)
#report test error rate
sink("../output/test_results/classificaton-tree-test-error-rate.txt")
print("Classification tree test error rate")
print(test_err_rate)
sink()
#create test confusion matrix
confusionMatrix <- table(ct_test_preds, real_test)
#report test confusion matrix
sink("../output/test_results/classification-tree-test-confusion-matrix.txt")
print("Classification Tree Confusion Matrix")
print(confusionMatrix )
sink()
#get sensitivity and specificity
sensitivity <- confusionMatrix[2, 2]/(confusionMatrix[2, 2] + confusionMatrix[1, 2])
specificity <- confusionMatrix[1, 1]/(confusionMatrix[1, 1] + confusionMatrix[2, 1])
#report sensitivity and specificity
sink("../output/test_results/classification-tree-sensitivity-specificity.txt")
print("Classification Tree Sensitivity:")
print(sensitivity)
print("Classification Tree Specificity:")
print(specificity)
sink()
#prepare roc cruve
ct_test_probs <- predict(prune_classification_tree, test_forest_preds)
ct_test_prediction <- prediction(ct_test_probs[,2], real_test)
ct_test_performance <- performance(ct_test_prediction,  measure = "tpr", x.measure = "fpr")
#prepare roc cruve
ct_test_probs <- predict(prune_classification_tree, test_tree_preds)
ct_test_prediction <- prediction(ct_test_probs[,2], real_test)
ct_test_performance <- performance(ct_test_prediction,  measure = "tpr", x.measure = "fpr")
#plot roc
pdf("../images/test_plots/classification-tree-test-roc-curve.pdf")
plot(ct_test_performance, main="Test ROC Classification Tree")
abline(a=0, b=1, lty=2)
dev.off()
#get test auc
test_auc <- performance(ct_test_prediction, measure="auc")@y.values[[1]]
#report test auc
sink("../output/classification-tree-test-auc.txt")
print("Classification Test AUC")
print(test_auc)
sink()
#load packages
library(randomForest)
library(ROCR)
library(caret)
library(stringr)
#read in clean data
train <- read.csv("../data/clean_train.csv", header = TRUE)
#remove numeric response column
train_forest <- train[, -ncol(train)]
#seperate predictors and response for cross validation
train_predictors <- train_forest[, -ncol(train_forest)]
train_response <- train_forest$Over50k
#set seed and run random forest cross validation with 10 folds
set.seed(100)
rf_cv <- rfcv(train_predictors, train_response, cv.fold = 5)
#get plot of cross validation error rate with number of variables used in random forest
pdf(file = "../images/training_plots/random-forest-cv-plot.pdf")
with(rf_cv, plot(n.var, error.cv, log="x", type="o", lwd=2, main = "CV Variables vs Error Rate", xlab = "Variables Used", ylab = "CV Error Rate"))
dev.off()
#get best number of variables to use
num_var <- as.numeric(names(which.min(rf_cv$error.cv)))
#issue with above is it will give full number of variables, which is what bagging is.
#We are going to try a different appraoch where number of trees and number of variables are tuned together
#create cv list
set.seed(200)
ntree <- c(50, 100, 500, 1000, 1500)
cv_list <- as.list(rep(0, 5))
names(cv_list) <- ntree
i <- 1
for (tree in ntree) {
tune_param <- tuneRF(train_predictors, train_response, ntreeTry = tree, trace = FALSE, plot = FALSE)
cv_list[[i]] <- tune_param
i <- i + 1
}
#create cv data frame
matrix <- matrix(rep(0, 3*5*3), nrow = 15, ncol = 3)
index <- 0
for (i in 1:5) {
cur_cv <- cv_list[i]
errs <- cv_list[[i]][,2]
for (j in 1:length(errs)){
cur_err <- cv_list[[i]][,2][[j]]
val <- str_sub(names(cv_list[[i]][,2][j]), 1, 1)
val <- as.numeric(val)
matrix[index + j, ] <- c(ntree[i], val, cur_err)
}
index <- index + j
}
cv_df <- data.frame(matrix)
names(cv_df) <- c("ntree", "num_var", "OOB")
#report cv data frame
sink("../output/training_results/random-forest-hyper-parameter-results.txt")
cv_df
sink()
#add colummn for plotting
cv_df$number_of_trees <- factor(cv_df$ntree)
#plot param tuning
pdf("../images/training_plots/random-forest-hyper-parameter-plot.pdf")
ggplot(data = cv_df, aes(x = num_var, y = OOB, color = number_of_trees)) + geom_line() + ggtitle("Random Forest Hyper Parameter Tuning")
dev.off()
#get hyper parameters
oobs <- cv_df$OOB
row_num <- as.numeric(which.min(oobs))
num_tree <- cv_df[row_num, 1]
num_var <- cv_df[row_num, 2]
#report hyper parameters
sink("../output/training_results/random-forest-best-hyper-parameters.txt")
print("Number of Trees")
print(num_tree)
print("")
print("Number of Variables in Random Selection")
print(num_var)
#model random forest with selected number of variables to use
set.seed(100)
random_forest <- randomForest(Over50k ~., data = train_forest,
ntree = num_tree, mtry = num_var,
importance = TRUE)
#report random forest result
sink("../output/training_results/random-forest-model.txt")
print(random_forest)
sink()
#report variable importance
sink("../output/training_results/random-forest-variable-importance.txt")
print(importance(random_forest))
sink()
#make variable importance plot
pdf("../images/training_plots/random-forest-variable-importance-plot.pdf")
varImpPlot(random_forest)
dev.off()
#get predictions of training data
rf_pred <- predict(random_forest, train_forest[, -ncol(train_forest)])
#find error rate
real_train <- train_forest$Over50k
err_rate <- mean(rf_pred != real_train)
#report error rate
sink("../output/training_results/random-forest-training-error-rate.txt")
print("Training error rate for Random Forest")
print(err_rate)
sink()
#get "yes" posterior props for ROC curve
probs <- as.vector(random_forest$votes[,2])
#get perdiction off of yes posterior probs
prediction <- prediction(probs, real_train)
#get performance
performance <- performance(prediction, measure = "tpr", x.measure = "fpr")
#plot ROC curve
pdf("../images/training_plots/random-forest-roc.pdf")
plot(performance, main="ROC Random Forest")
abline(a=0, b=1, lty = 2)
dev.off()
#get auc
auc <- performance(prediction, measure="auc")@y.values[[1]]
#report auc
sink("../output/training_results/random-forest-auc.txt")
print("AUC of training set for Random Forest")
print(auc)
sink()
#report training confusion matrix
sink("../output/training_results/random-forest-confusion-matrix.txt")
print(random_forest$confusion)
sink()
#read in data
test <- read.csv("../data/clean_test.csv", header = TRUE)
#prepare test set
test_forest <- test[, -ncol(test)]
#seperate predictors and responses
test_forest_preds <- test_forest[, -ncol(test_forest)]
real_test <- test_forest$Over50k
#get test error rate
rf_test_preds <- predict(random_forest, test_forest_preds)
test_err_rate <- mean(rf_test_preds != real_test)
#report test error rate
sink("../output/test_results/random-forest-test-error-rate.txt")
print("Random Forest Test Error Rate")
print(test_err_rate)
sink()
#get confusion matrix
confusionMatrix <- table(rf_test_preds, real_test)
#report confusion matrix
sink("../output/test_results/random-forest-confusion-matirx.txt")
print("Random Forest Confusion Matrix")
print(confusionMatrix)
sink()
#get sensitivity
sensitivity <- confusionMatrix[2, 2]/(confusionMatrix[2, 2] + confusionMatrix[1, 2])
#get specificity
specificity <- confusionMatrix[1, 1]/(confusionMatrix[1, 1] + confusionMatrix[2, 1])
#report sensitivity and specificity
sink("../output/test_results/random-forest-sensitivity-specificity.txt")
print("Random Forest Sensitivity:")
print(sensitivity)
print()
print("Rnaomd Forest Specificity:")
print(specificity)
sink()
#prepare ROC plot
test_probs <- predict(random_forest, test_forest_preds, type = "prob")
test_prediction <- prediction(test_probs[,2], real_test)
test_performance <- performance(test_prediction,  measure = "tpr", x.measure = "fpr")
#plot test ROC
pdf("../images/test_plots/random-forest-test-ROC.pdf")
plot(test_performance, main="Test ROC Random Forest")
abline(a=0, b=1, lty=2)
dev.off()
#get auc of test
test_auc <- performance(test_prediction, measure="auc")@y.values[[1]]
#report auc
sink("../output/test_results/random-forest-test-auc.txt")
print("Random Forest Test AUC")
print(test_auc)
sink()
#report sensitivity and specificity
sink("../output/test_results/random-forest-sensitivity-specificity.txt")
print("Random Forest Sensitivity:")
print(sensitivity)
print("")
print("Rnaomd Forest Specificity:")
print(specificity)
sink()
#prepare ROC plot
test_probs <- predict(random_forest, test_forest_preds, type = "prob")
test_prediction <- prediction(test_probs[,2], real_test)
test_performance <- performance(test_prediction,  measure = "tpr", x.measure = "fpr")
#plot test ROC
pdf("../images/test_plots/random-forest-test-ROC.pdf")
plot(test_performance, main="Test ROC Random Forest")
abline(a=0, b=1, lty=2)
dev.off()
#report auc
sink("../output/test_results/random-forest-test-auc.txt")
print(test_auc)
print("Random Forest Test AUC")
#get auc of test
test_auc <- performance(test_prediction, measure="auc")@y.values[[1]]
sink()
library(rpart)
names <- classification_tree_cv_default$k
values <- classification_tree_cv_default$dev
names(values) <- names
cost <- as.numeric(names(which.min(values)))
cost
library(rpart)
names <- classification_tree_cv_default$k
values <- classification_tree_cv_default$dev
names(values) <- names
cost <- as.numeric(names(which.min(values)))
cost
library(rpart)
names <- classification_tree_cv_default$k
values <- classification_tree_cv_default$dev
names(values) <- names
cost <- as.numeric(names(which.min(values)))
names
library(rpart)
names <- classification_tree_cv_default$k
values <- classification_tree_cv_default$dev
names(values) <- names
cost <- as.numeric(names(which.min(values)))
print(names)
cost
classification_tree_cv_default$k
train <- read.csv("/Users/jackmoorer/Stat154/Projects/Project/data/clean_train.csv", header = TRUE)
test <- read.csv("/Users/jackmoorer/Stat154/Projects/Project/data/clean_test.csv", header = TRUE)
library(ISLR)
library(tree)
library(rpart)
train_tree <- train[,-ncol(train)]
classification_tree <- tree(Over50k ~., data = train_tree)
summary(classification_tree)
set.seed(100)
classification_tree_cv <- cv.tree(classification_tree, FUN = prune.misclass)
library(ggplot2)
Size <- classification_tree_cv_misclass$size
K <- classification_tree_cv_misclass$k
Dev <- classification_tree_cv_misclass$dev
Misclass <- data.frame(Size, K, Dev)
ggplot(data = Misclass, aes(x = Size, y = Dev)) + geom_point() + geom_line() + ggtitle("Size of Tree vs Error for Misclass Method")
set.seed(200)
classification_tree_cv_default <- cv.tree(classification_tree, FUN = prune.tree)
classification_tree_cv_default
Size <- classification_tree_cv_default$size
K <- classification_tree_cv_default$k
Dev <- classification_tree_cv_default$dev
default <- data.frame(Size, K, Dev)
ggplot(data = default, aes(x = Size, y = Dev)) + geom_point() + geom_line() + ggtitle("Size of Tree vs Error for Default Method")
ggplot(data = default, aes(x = K, y = Dev)) + geom_point() + geom_line() + ggtitle("Cost-Complexity vs Error for Default Method")
set.seed(200)
classification_tree_cv_default <- cv.tree(classification_tree, FUN = prune.tree)
classification_tree_cv_default
Size <- classification_tree_cv_default$size
K <- classification_tree_cv_default$k
Dev <- classification_tree_cv_default$dev
default <- data.frame(Size, K, Dev)
ggplot(data = default, aes(x = Size, y = Dev)) + geom_point() + geom_line() + ggtitle("Size of Tree vs Error for Default Method")
ggplot(data = default, aes(x = K, y = Dev)) + geom_point() + geom_line() + ggtitle("Cost-Complexity vs Error for Default Method")
library(rpart)
names <- classification_tree_cv_default$k
values <- classification_tree_cv_default$dev
names(values) <- names
cost <- as.numeric(names(which.min(values)))
cost
print(cost)
Size <- classification_tree_cv_default$size
K <- classification_tree_cv_default$k
Dev <- classification_tree_cv_default$dev
default <- data.frame(Size, K, Dev)
default
library(ggplot2)
Size <- classification_tree_cv_misclass$size
K <- classification_tree_cv_misclass$k
Dev <- classification_tree_cv_misclass$dev
Misclass <- data.frame(Size, K, Dev)
Misclass
cost <- 0
rpart <- rpart(Over50k ~., data = train_tree, control = rpart.control(cp = cost))
prune_classification_cost <- prune.tree(class_tree, k = cost)
cost <- 0
rpart <- rpart(Over50k ~., data = train_tree, control = rpart.control(cp = cost))
prune_classification_cost <- prune.tree(classification_tree, k = cost)
rpart
plot(rpart)
plot(prune_classification_cost)
plot(prune_classification_cost, pretty = 0)
plot(prune_classification_cost)
text(prune_classification_cost ,pretty =0)
plot(rpart)
text(rpart, pretty = 0 )
