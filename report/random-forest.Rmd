---
title: "random-forest"
author: "JackMoorer"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r}
train <- read.csv("/Users/jackmoorer/Stat154/Projects/Project/data/clean_train.csv", header = TRUE)
test <- read.csv("/Users/jackmoorer/Stat154/Projects/Project/data/clean_test.csv", header = TRUE)
```

```{r}
library(ggplot2)
library(randomForest)
```

##Train Set
```{r}
train_forest <- train[, -ncol(train)]
```

First I tried just the basic random forest to see the results
```{r}
set.seed(200)
rf <- randomForest(Over50k ~., data = train_forest, imporatance = TRUE)
```

```{r}
rf$confusion
```

```{r}
importance(rf)
```
```{r}
varImpPlot(rf)
```

```{r}
train_predictors <- train_forest[, -ncol(train_forest)]
```

```{r}
train_response <- train_forest$Over50k
```

Next I did cross validation to tune the number of predictors used while building a tree
```{r}
set.seed(200)
rf_cv <- rfcv(train_predictors, train_response, cv.fold = 5)
```

The plot below shows the best number of predictors to use is all 13.
```{r}
with(rf_cv, plot(n.var, error.cv, log="x", type="o", lwd=2, main = "CV Variables vs Error Rate", xlab = "Variables Used", ylab = "CV Error Rate"))
```
```{r}
num_var <- as.numeric(names(which.min(rf_cv$error.cv)))
print(num_var)
```
I am going to run the random forest using this parameter:
```{r}
set.seed(100)
random_forest <- randomForest(Over50k ~., data = train_forest, mtry = num_var, importance = TRUE)
```

```{r}
print(random_forest)
```

```{r}
importance(random_forest)
```
```{r}
varImpPlot(random_forest)
```

```{r}
rf_pred <- predict(random_forest, train_forest[, -ncol(train_forest)])
```


```{r}
real_train <- train_forest$Over50k
err_rate <- mean(rf_pred != real_train)
err_rate
```

```{r}
library(ROCR)
```

```{r}
probs <- as.vector(random_forest$votes[,2])
```


```{r}
prediction <- prediction(probs, real_train)
performance <- performance(prediction, measure = "tpr", x.measure = "fpr")
```

```{r}
plot(performance, main="ROC Random Forest")
abline(a=0, b=1, lty = 2)
```

```{r}
auc <- performance(prediction, measure="auc")@y.values[[1]]
auc
```

```{r}
print(random_forest$confusion)
```

##Test Set
```{r}
test_forest <- test[, -ncol(test)]
```

```{r}
test_forest_preds <- test_forest[, -ncol(test_forest)]
real_test <- test_forest$Over50k
```

```{r}
rf_test_preds <- predict(random_forest, test_forest_preds)
err_rate <- mean(rf_test_preds != real_test)
err_rate
```

```{r}
confusionMatrix <- table(rf_test_preds, real_test)
confusionMatrix 
```
```{r}
sensitivity <- confusionMatrix[2, 2]/(confusionMatrix[2, 2] + confusionMatrix[1, 2])
sensitivity
```
```{r}
specificity <- confusionMatrix[1, 1]/(confusionMatrix[1, 1] + confusionMatrix[2, 1])
specificity
```


```{r}
test_probs <- predict(random_forest, test_forest_preds, type = "prob")
```

```{r}
test_prediction <- prediction(test_probs[,2], real_test)
test_performance <- performance(test_prediction,  measure = "tpr", x.measure = "fpr")
```


```{r}
plot(test_performance, main="Test ROC Random Forest")
abline(a=0, b=1, lty=2)
```

```{r}
auc <- performance(test_prediction, measure="auc")@y.values[[1]]
auc
```













