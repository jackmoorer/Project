---
title: "Stat 154 Final Project Report"
author: "Jack Moorer and Jaime Pimentel"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r}
#read in the clean data
train <- read.csv("/Users/jackmoorer/Stat154/Projects/Project/data/clean_train.csv", header = TRUE)
test <- read.csv("/Users/jackmoorer/Stat154/Projects/Project/data/clean_test.csv", header = TRUE)
```


#Building Models

##Clasification Tree

###Build the Tree

There seems to be two main ways to build a classification tree in r. The first is to use the package rpart, which was used in the examples in APM. The second way, which is the method they used in ISL and we used in lab, was using the tree package. After looking at booth methods, we decided to build our classification tree using the method form the tree package.

```{r}
#load packages
library(ISLR)
library(tree)
library(rpart)
```

The tree method allows for categorical response variables and categorical predictor variables, so we don’t we use the numeric over 50k indicator variable or transform our predictors into dumby variables.

```{r}
#remove numeric reponse column
train_tree <- train[,-ncol(train)]
```

I am going to start out by building a classification tree using the default tree function, just to get an idea of what is happening.

```{r}
#default classification tree
classification_tree <- tree(Over50k ~., data = train_tree)
summary(classification_tree)
```

I can now move on to tuning optimal parameters. From what I can see there are to main ways to tune hyperparameters using the tree package. . Both ways use cv.tree from the tree package, with the only difference being the prune function to perform cross validation. I am going to start by using prune.misclass, which looks at the parameters by looking at the misclassification rate of the tree with those parameters. The parameter we can tune is one of the cost-complexity parameter, k, or the number of terminal nodes in the tree, size. I am going to focus on the number of terminal nodes, size, as the parameter to tune, but I will still show the results of the cost-complexity parameter as well.

```{r}
#tune parameters
set.seed(100)
classification_tree_cv <- cv.tree(classification_tree, FUN = prune.misclass)
```

In cv.tree, the performance of the parameter is represented with dev, which looks at the misclassification rate of the tree with the given parameters. We can plot the results of the different terminal node sizes or cost-complexity parameters with dev.

```{r}
#look at data frame of cv results
library(ggplot2)
Size <- classification_tree_cv$size
K <- classification_tree_cv$k
Dev <- classification_tree_cv$dev
Misclass <- data.frame(Size, K, Dev)
Misclass
```

```{r}
#plot cv results of number of terminal nodes
ggplot(data = Misclass, aes(x = Size, y = Dev)) + geom_point() + geom_line() + ggtitle("Size of Tree vs Error for Misclass Method")
```
```{r}
#plot cv results of number cost-complexity
ggplot(data = Misclass, aes(x = K, y = Dev)) + geom_point() + geom_line() + ggtitle("Cost-Complexity vs Error for Misclass Method")
```

We could use k = 0 as a cost complexicity hyper-parameter. However, if we look at number of trees using 5 or 8 both give us the same minimum. We can also look at a different type of cross validation approach using the default prune.tree function from cv.tree(). In theory I could just pick the smaller value of 5, since an increased number of terminal nodes may overfit the data, however, I am going to compare the results of this tuning process to one using prune.tree has the function in cv.tree.

```{r}
#cv using prune.tree
set.seed(200)
classification_tree_cv_default <- cv.tree(classification_tree, FUN = prune.tree)
```


```{r}
#creat data frame of results
Size <- classification_tree_cv_default$size
K <- classification_tree_cv_default$k
Dev <- classification_tree_cv_default$dev
default <- data.frame(Size, K, Dev)
default
```

```{r}
#plot cv results for size
ggplot(data = default, aes(x = Size, y = Dev)) + geom_point() + geom_line() + ggtitle("Size of Tree vs Error for prune.tree Method")
```

```{r}
#plot cv results for cross complexity
ggplot(data = default, aes(x = K, y = Dev)) + geom_point() + geom_line() + ggtitle("Cost-Complexity vs Error for Default Method")
```

From both of these results it seems like minimum cost-complexity and maximum size is ideal for our tuned parameters. I am going to use size = 8 as the optical parameter.

```{r}
#get best size parameter from cross validation
names <- classification_tree_cv_default$size
values <- classification_tree_cv_default$dev
names(values) <- names
size <- as.numeric(names(which.min(values)))
```

I can now build an optimal classification tree using the best size parameter using prune.misclass.

```{r}
#build classification tree
set.seed(4)
prune_classification_tree <- prune.misclass(classification_tree, best = size)
```

```{r}
#let's look at the tree
prune_classification_tree
```

```{r}
#lets look at the structure of the tree
plot(prune_classification_tree)
text(prune_classification_tree, pretty = 0)
```


Based on the tree structure, the most important variable is relationship status, followed by capital gain, and then education, then finally occupation. These are the only variables used to make the tree.  However, there are some further insights to be had. The most important categories that split the data in relationship are not-in-family, other-relative, own-child, and unmarried. Next is capital gain and education level, with a capital gain boundary of 7055.5 as the next best split of the data. The important features for education are the levels corresponding to non-college graduates. The tree is implying that the third most important feature for over 50k classification is whether of not you got at least a college degree, and the results imply if you did not get at least a college degree you will not make over 50k. Finally, occupation is the last varibale used in the tree.

I could not find how to report importance statists of variables from classification trees using the tree library, only the rpart library.

###Training Results

Now I can look at the training accuracy rate. I also want to look at the training confusion matrix

```{r}
#compute confusion matrix
real_train <- train_tree$Over50k
train_preds <- predict(prune_classification_tree, train_tree, type = "class")
table(train_preds, real_train)
```

```{r}
#get error rate of classification tree
err_rate <- mean(train_preds != real_train)
err_rate
```

```{r}
#get accuracy
training_accuracy_class_tree = 1 - err_rate
```

The training accuracy rate for the classification tree we built is 'r training_accuracy_class_tree'.


Now I can plot the ROC curve and find the AUC of the training data.

```{r}
#load package
library(ROCR)
```

```{r}
#prepare roc plot
train_probs <- predict(prune_classification_tree, train_tree)
train_prediction <- prediction(train_probs[,2], real_train)
train_performance <- performance(train_prediction, measure = "tpr", x.measure = "fpr")
```

```{r}
#plot roc
plot(train_performance, main = "Train ROC Curve for Classification Tree")
abline(a=0, b=1, lty=2)
```

```{r}
#get train auc
class_tree_train_auc <- performance(train_prediction, measure="auc")@y.values[[1]]
class_tree_train_auc
```

The training auc for the classification tree is 'r class_tree_train_auc'.


##Random Forest



###Build Model
```{r}
#load packages
library(randomForest)
library(caret)
```

```{r}
#prepare data
train_forest <- train[, -ncol(train)]
```

We are going to build a random forest using the randomForest package in r. The important distinction between random forest and bagged trees is that random forest takes a random subset of the features to build when splitting each node, unlike bagging, that uses all features. This means on of the most important parameters to tune for our random forest is the number of features to use, or mtyr in r. The randomForest package has a method called rfcv that performs cross validation in order to tune the value of mtyr. There are two issues with the rfcv function, for one, it allows for the maximum number of features as a possible value for mtyr, but we specifically don’t want to use all of the features to make our ranfom forest distinct from our bagged tree. The second issue with rfcv is that it is hard to tune the number of trees used during the random forest estimation. I attempted to loop though the number of trees, then run rfcv on each number of trees, but this was taking a very long time. You can see the details and results of me trying rfcv in the R script file and results and plot files, or look at the separate random-forest.Rmd file in the sub-reports folder.

Instead of rfcv I am going to use the function tuneRF to tune the number of trees and number of features to use for the random forest.

```{r}
#seperate predictors and response
train_predictors <- train_forest[, -ncol(train_forest)]
train_response <- train_forest$Over50k
```

```{r}
#set seed
set.seed(200)
#number of trees to look at
ntree <- c(50, 100, 500, 1000, 1500)
#how I will store the results
cv_list <- as.list(rep(0, 5))
names(cv_list) <- ntree
i <- 1
#loop through number of trees and run tuneRF to look at combination of mtyr and number of trees
for (tree in ntree) {
  tune_param <- tuneRF(train_predictors, train_response, ntreeTry = tree, trace = FALSE, plot = FALSE)
  cv_list[[i]] <- tune_param
  i <- i + 1
}
```
```{r}
#prepare cv results for plotting
library(stringr)
matrix <- matrix(rep(0, 3*5*3), nrow = 15, ncol = 3)
index <- 0
for (i in 1:5) {
  cur_cv <- cv_list[i]
  errs <- cv_list[[i]][,2]
  for (j in 1:length(errs)){
    cur_err <- cv_list[[i]][,2][[j]]
    val <- str_sub(names(cv_list[[i]][,2][j]), 1, 1)
    val <- as.numeric(val)
    matrix[index + j, ] <- c(ntree[i], val, cur_err)
  }
  index <- index + j
}
cv_df <- data.frame(matrix)
names(cv_df) <- c("ntree", "num_var", "OOB")
cv_df
```

Here is the paramater tuning results grouped by number of trees.
```{r}
cv_df$number_of_trees <- factor(cv_df$ntree)
ggplot(data = cv_df, aes(x = num_var, y = OOB, color = number_of_trees)) + geom_line() + ggtitle("Random Forest Hyper Parameter Tuning Grouped By Number of Trees")
```

Here is the paramater tuning results grouped by number of variables.
```{r}
cv_df$mtyr <- factor(cv_df$num_var)
ggplot(data = cv_df, aes(x = ntree, y = OOB, color = mtyr)) + geom_line() + ggtitle("Random Forest Hyper Parameter Tuning Grouped By mtyr")
```

```{r}
oobs <- cv_df$OOB
row_num <- as.numeric(which.min(oobs))
num_tree <- cv_df[row_num, 1]
num_var <- cv_df[row_num, 2]
```

The best parameter combination based on our hyperparameter tuning restuls are number of trees is 'r num_tree' and number of features is 'r num_var'. These are based on the lowest OOB error rate during the cross validation process.

Now I can build the random forest using these parameters.
```{r}
set.seed(100)
random_forest <- randomForest(Over50k ~., data = train_forest, ntree = num_tree, mtry = num_var, importance = TRUE)
```

Now I can look at the variable importance using the importance function. Variable importance will be based on the greatest mean decreases gini.
```{r}
#look at variable importance of random forest
var_imp <- data.frame(importance(random_forest))
var_imp
```

```{r}
#lets plot the variable importance based on mean decrease gini and mean decrease accuracy
varImpPlot(random_forest)
```

I am going to base varaible importance on mean decrease gini. Based on the plots above the most important features are capital_gain, relationship, education, matrical_status, occupation, and age, in that order. The variable importance statistics are:

```{r}
var_imp_best_6 <- var_imp[order(var_imp$MeanDecreaseGini, decreasing = TRUE)[1:6],]
var_imp_best_6
```

The variable importance statistics are:

* capitcal_gain: 1080.7801

* relationship: 1010.8085

* education: 963.7646

* marital_status: 913.8188

* occupation: 862.5448

* age: 815.9970


###Training Results

```{r}
#prediction the values of the training set
rf_train_pred <- predict(random_forest, train_forest[, -ncol(train_forest)])
```

```{r}
real_train <- train_forest$Over50k
rf_train_err_rate <- mean(rf_train_pred != real_train)
rf_train_err_rate
```

```{r}
rf_train_acc <- 1 - rf_train_err_rate
rf_train_acc
```

The training data accuracy for random forest is 'r rf_train_acc'.

Here is also the Random Forest training confusion matrix.

```{r}
print(random_forest$confusion)
```

Now we can look at the training ROC and AUC of the Random Forest.

```{r}
#prepare trianing roc
rf_train_probs <- as.vector(random_forest$votes[,2])
rf_prediction <- prediction(rf_train_probs, real_train)
rf_performance <- performance(rf_prediction, measure = "tpr", x.measure = "fpr")
```

```{r}
#plot training roc
plot(rf_performance, main="Training ROC Random Forest")
abline(a=0, b=1, lty = 2)
```

```{r}
train_rf_auc <- performance(rf_prediction, measure="auc")@y.values[[1]]
train_rf_auc
```

The Training AUC of the Random Forest is 'r train_rf_auc'.

#Testing Models

We are going to look at the performance of all of the methods, and then report the best at the end.

##Classification Tree
This section will be based around looking at the test performance of the classificaiton tree we build. 

###Error Rate

```{r}
#prepare test data
test_tree <- test[, -ncol(test)]
test_tree_preds <- test_tree[, -ncol(test_tree)]
real_test <- test_tree$Over50k
```


```{r}
#get error rate
ct_test_preds <- predict(prune_classification_tree, test_tree_preds, type = "class")
ct_test_err_rate <- mean(ct_test_preds != real_test)
ct_test_err_rate
```

```{r}
#get test accuracy
ct_test_acc <- 1 - ct_test_err_rate
ct_test_acc
```

The test accuracy of our classification tree is 'r ct_test_acc'.


###Confusion Matrix
```{r}
#get classification tree confusion matrix
class_tree_confusionMatrix <- table(ct_test_preds, real_test)
class_tree_confusionMatrix 
```

###Specificity and Sensitivity

```{r}
#calculate sensitivity
ct_sensitivity <- class_tree_confusionMatrix[2, 2]/(class_tree_confusionMatrix[2, 2] + class_tree_confusionMatrix[1, 2])
ct_sensitivity
```

```{r}
#calculate specificity
ct_specificity <- class_tree_confusionMatrix[1, 1]/(class_tree_confusionMatrix[1, 1] + class_tree_confusionMatrix[2, 1])
ct_specificity
```

The classification tree sensitivity is 'r ct_sensitivity' and the classification tree specificity is 'r ct_specificity'. We have a low sensitivity and high specificity.

###ROC and AUC
```{r}
#prepare roc curve
ct_test_probs <- predict(prune_classification_tree, test_tree_preds)
ct_test_prediction <- prediction(ct_test_probs[,2], real_test)
ct_test_performance <- performance(ct_test_prediction,  measure = "tpr", x.measure = "fpr")
```


```{r}
#plot roc curve
plot(ct_test_performance, main="Test ROC Classification Tree")
abline(a=0, b=1, lty=2)
```

```{r}
#report test auc
ct_test_auc <- performance(ct_test_prediction, measure="auc")@y.values[[1]]
ct_test_auc
```

The test auc for the classification tree is 'r ct_test_auc'.


##Random Forest

```{r}
#prepare data
test_forest <- test[, -ncol(test)]
test_forest_preds <- test_forest[, -ncol(test_forest)]
real_test <- test_forest$Over50k
```

###Error Rate
```{r}
#get test error rate
rf_test_preds <- predict(random_forest, test_forest_preds)
rf_test_err_rate <- mean(rf_test_preds != real_test)
rf_test_err_rate
```

```{r}
#get test acc
rf_test_acc <- 1 - rf_test_err_rate
rf_test_acc
```

The random forest test accuracy is 'r rf_test_acc'.


###Confusion Matrix
```{r}
#get test confusion matrix
rf_test_confusionMatrix <- table(rf_test_preds, real_test)
rf_test_confusionMatrix 
```

###Specificity and Sensitivity

```{r}
# get test sensitivity
rf_test_sensitivity <- rf_test_confusionMatrix[2, 2]/(rf_test_confusionMatrix[2, 2] + rf_test_confusionMatrix[1, 2])
rf_test_sensitivity
```


```{r}
#get test specificity
rf_test_specificity <- rf_test_confusionMatrix[1, 1]/(rf_test_confusionMatrix[1, 1] + rf_test_confusionMatrix[2, 1])
rf_test_specificity
```

The random forest sensitivity is 'r rf_test_sensitivity' and specificity is 'r rf_test_specificity'. We have a pretty high specificity, but a pretty low sensitivity.

###ROC and AUC
```{r}
#prepare roc cruve
rf_test_probs <- predict(random_forest, test_forest_preds, type = "prob")
rf_test_prediction <- prediction(rf_test_probs[,2], real_test)
rf_test_performance <- performance(rf_test_prediction,  measure = "tpr", x.measure = "fpr")
```

```{r}
#plot random forest roc
plot(rf_test_performance, main="Test ROC Random Forest")
abline(a=0, b=1, lty=2)
```

```{r}
#get test auc
rf_test_auc <- performance(rf_test_prediction, measure="auc")@y.values[[1]]
rf_test_auc
```

The random forest test AUC was 'r rf_test_auc'.
